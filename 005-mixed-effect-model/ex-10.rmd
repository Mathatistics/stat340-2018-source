## Exercise 10


```{r, echo = F, message=FALSE}
library(mixlm)
library(car)
load('exer.rdata')
```

### Ex-1: Exercises and diet - repeated measurements

The data and some examples are found at [statistics.ats.ucla.edu](http://statistics.ats.ucla.edu/stat/r/seminars/Repeated_Measures/repeated_measures.htm)

The data called exer, consists of people who were randomly assigned to two different diets: low-fat and not low-fat and three different types of exercise: at rest, walking leisurely and running. Their pulse rate was measured at three different time points during their assigned exercise: at 1 minute, 15 minutes and 30 minutes.

Upon downloading the data, you can load and plot the data by

```{r}
plot(exer, outer = TRUE, aspect = "fill", key = NULL)
```

You will see six plots with heading's like `2:1` which means `diet=2` and exercises `type=1`.
In each plot we have three repeated measurements for each of 5 people.

In these exercises you will be asked to answer some questions. Use the R-outputs to the extent your find it necessary.

a. Below is a output from a linear model fitted to the data. State the model that has been fitted and the assumptions made. Which parametrization is used?

```{r}
options(contrasts = c("contr.sum", "contr.poly"))
mod1 <- lm(pulse ~ (time + diet + exertype) ^ 2, data = exer)
summary(mod1)
anova(mod1)
```

<div class="ans">

We can write the model in this way:

$$
\begin{aligned}
\mathtt{pulse}_{ijkl} = \mu + &\beta\;\mathtt{time}_{l} + \mathtt{diet}_j + \mathtt{exertype}_k + \beta\;\mathtt{time}_{l}\cdot\mathtt{exertype}_{k} +  \\
&\beta\;\mathtt{time}_{l}\cdot\mathtt{diet}_{jl} +  \mathtt{diet}_j\cdot\mathtt{exertype}_k + \epsilon_{ijkl}
\end{aligned}
$$

Where we assume that the errors are identically and independently distributed (iid) as: 

$\epsilon_{ijkl} \sim N(0, \sigma^2)$ for all 

  - $j = 1, 2$  (For Diet)
  - $k = 1, 2, 3$ (For Exercise Type) and
  - $l = 1, 2, 3$ (For individual time points)


In orther words it is a linear model with main effects of time, diet and exercise type,
plus all second-order interactions between these. 

Remark: From the ANOVA-table output we may observe that time
has only one degree of freedom (thus one parameter is estimated for the effect of time)
which means that time is assumed to be a continuous effect. If time alternatively was
modelled as a categorical variable, it would have had 2 degrees of freedom in the ANOVA table
(the number of time-levels minus one).

Further, a sum-to-zero parametrization is used for all categorical effects, for instance,
we assume that the two diet effects sum to zero, hence the estimate of `diet(2)` is:
`diet(2) = -diet(1) = -(-0.1778) = 0.1778`

The three exercise type effects likewise, that is:
`exertype(3) = -(exertype(1) + exertype(2)) = -(1.2333 + 4.0000) = -5.233`

For interactions between categorical effects, the effects sum to zero if we keep one variable
fixed at one level and sum across the levels of the other. E.g. for diet 1 the interaction effects with exercise types 1, 2 and 3 sum to zero, and so on. That is:
we would from the ouput find:

`diet(1):exertype(3) = -(diet(1):exertype(1) + diet(1):exertype(2)) = -(2.2444 + 2.0111) = -4.256`

However, it also means that for fixed level 3 for exercise type, the diet effects should sum to zero. That is:

`diet(2):exertype(3) = - diet(1):exertype(3) = -(-4.256) = 4.256`

For interactions between the continuous variable time and a catergorical variable, the effects sum to zero across the levels of the categorical variable if we keep time fixed.

Hence:
`time:diet(2) = -time:diet(1) = -(-1.783) = 1.783`

and
`time:exertype(3) = -(time:exertype(1) + time:exertype(2)) = -(-5.05 - 4.25)=  9.30`

</div>

b. Write up the fitted model for pulse as a function of time for the special case of `diet=1`
and `exertype=1`. Do the same for `diet=2` and `exertype=3`. 

<div class="ans">

The fitted model for `diet=1` and `exertype=1` is found directly by selecting the estimated 
coefficients from the output for which diet=1 and exertype=1:

$$
\begin{aligned}
\hat{\mathtt{pulse}} &= `r mod1$coef["(Intercept)"]` + `r mod1$coef["time"]` \mathtt{time} `r round(mod1$coef["diet(1)"], 3)` + `r round(mod1$coef["exertype(1)"], 3)``r round(mod1$coef["time:diet(1)"], 3)` \mathtt{time} `r round(mod1$coef["time:exertype(1)"], 3)` \mathtt{time} + `r round(mod1$coef["diet(1):exertype(1)"], 3)` \\
&= `r mod1$coef["(Intercept)"] + round(mod1$coef["diet(1)"], 3) + round(mod1$coef["exertype(1)"], 3) + round(mod1$coef["diet(1):exertype(1)"], 3)` `r mod1$coef["time"] + round(mod1$coef["time:diet(1)"], 3) + round(mod1$coef["time:exertype(1)"], 3)` \mathtt{time}
\end{aligned}
$$

Similarly, using the estimates found above through the sum-to-zero parametrizations, the fitted model for `diet=2` and `exertype=3` becomes:

```{r, echo = F}
coef <- mod1$coefficients
```

$$
\begin{aligned}
\hat{\mathtt{pulse}} &= `r round(coef["(Intercept)"] - sum(coef[c("diet(1)", "exertype(1)", "exertype(2)")]) + sum(coef[c("diet(1):exertype(1)", "diet(1):exertype(2)")]), 3)` + `r round(coef[which(grepl("time", names(coef)))][1] - sum(coef[which(grepl("time", names(coef)))][-1]), 3)` \mathtt{time}
\end{aligned}
$$

</div>

c. What is the estimated noise variance? Give an interpretation of the estimated time effect (5.65) for a person with limited statistics knowledge.

<div class="ans">

The estimated noise variance is $\hat{\sigma}^{2}$ = MSE= `r round(summary(mod1)$sigma ^ 2, 3)`. The estimate of `time` variable which is `r mod1$coef["time"]` refers to the average change in pulse per unit increase in time, for persons being part of this study. 

</div>

d. The anova table gives significant interaction between time and exertype. Use the graphs above to explain why there is an interaction between these two variables.

<div class="ans">

Since the interaction term is significant, for each individual, the line representing the pulse measured at three time points are inclined different (not parallel) which also says that the pulse for an individual under different exercise type changes differently over time.

</div>

e. Are there any problems related to the model assumptions for the model fitted in a. ?

<div class="ans">

Model 1 is based on the assumption that all error terms (and hence all observations) are
independent. Since we know that there are three repeated observations on each person, we
should expect that observations made on the same person are dependent. For instance, 
a person with low puls at time 1 is expected to also have (relatively) low puls at time 
points2 and 3. The Figure below confirms this tendency, especially for exercise types 1 and 2.
We see that the residuals of any given person tend to be either above or below the fitted lines.

```{r, echo = FALSE, message=FALSE}
require(ggplot2)
mod1.fit <- cbind(exer, 
                  fitted = mod1$fitted.values, 
                  indiv = factor(rep(rep(1:5, each = 3), 6)))
ggplot(mod1.fit, aes(time, pulse, fill = indiv)) + geom_point(shape = 21) + geom_line(aes(y = fitted)) + facet_grid(diet ~ exertype, labeller = labeller(diet = label_both, exertype = label_both))
```

</div>

f. Another analysis was performed as shown below. Describe the difference between mod2 and mod1. Are there any extra parameters estimated? Why is mod2 probably a more reasonable model than mod1?

```{r}
library(nlme)
mod2 <- lme(pulse ~ (time + diet + exertype) ^ 2, random = ~1 | id, data = exer)
summary(mod2)
```

<div class="ans">

Model 2 assumes a random intercept for each individual and the random intercept is assumed to follow $N(0, \sigma_\alpha^2)$. So, $\sigma_\alpha$ is an extra parameters to be estimated. From model 2, we can obtain a separate fitted line for each individual, which is more resonable and which take better into account that we may have dependence between observations made on the same person.

```{r, echo = FALSE, message=FALSE}
mod2.fit <- cbind(exer, `colnames<-`(mod2$fitted, c('fixed', 'random')), 
                  indiv = factor(rep(rep(1:5, each = 3), 6)))
ggplot(mod2.fit, aes(time, pulse, fill = indiv)) + geom_point(shape = 21) + geom_line(aes(y = random, color = indiv)) + facet_grid(diet ~ exertype, labeller = labeller(diet = label_both, exertype = label_both))
```

</div>

g. For models containing random effects there is a function `ranef` in the `nlme`-package
which displays the predicted random effects. The first 5 random effects are shown below
with "rowname" equal to the id number of a given person. Explain what it means that person
with id=4 has a random effect of `r round(ranef(mod2)[1,], 3)`.

```{r}
ranef(mod2)[1:5,,drop = F]
```

<div class="ans">

This means that person with `id=4` has on average `r round(abs(ranef(mod2)[1,]), 3)` units lower pulse than the average person, keeping all other variables fixed. 

</div>

h. A third model was analysed by:

```{r}
library(nlme)
mod3 <- lme(pulse ~ (time + diet + exertype) ^ 2, random = ~1 | id, correlation = corAR1(), data = exer)
summary(mod3)
```

How does mod 3 differ from mod2 in its model assumptions? Are there any extra parameters estimated? 

<div class="ans">

Apart from all the assumption in Model 2, Model 3 also assumes a first order autocorrelation between the error terms. In simple words, observation far from each other tend to be less correlated.

</div>

i. Which of mod2 and mod3 is to prefer. Explain why based on the output below.

```{r}
anova(mod2, mod3)
```

<div class="ans">

The `anova()` function used here returns the result of a likelihood-ratio test (a chi-square test). The null-hypothesis for the test is that the two models fit the data equally well. The p-value is large, and we retain the null-hypothesis. Hence, the extra refinement introduced
in mod3 is not improving the model fit significantly, and we may keep the simpler mod2.

</div>
