<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Applied Methods in Statistics</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This contains the weekly exercises related to Applied Methods in Statistics (STAT340)">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Applied Methods in Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This contains the weekly exercises related to Applied Methods in Statistics (STAT340)" />
  <meta name="github-repo" content="therimalaya/stat340" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Applied Methods in Statistics" />
  
  <meta name="twitter:description" content="This contains the weekly exercises related to Applied Methods in Statistics (STAT340)" />
  

<meta name="author" content="Solve Sæbø">


<meta name="date" content="2017-01-01">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="exercise-6.html">
<link rel="next" href="generalized-linear-model.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Methods in Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Practical Information</a><ul>
<li class="chapter" data-level="" data-path="reference-books.html"><a href="reference-books.html"><i class="fa fa-check"></i>Reference Books</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i>Getting Started</a><ul>
<li class="chapter" data-level="" data-path="create-new-project.html"><a href="create-new-project.html"><i class="fa fa-check"></i>Create New Project</a></li>
<li class="chapter" data-level="" data-path="exercise-1-data-import.html"><a href="exercise-1-data-import.html"><i class="fa fa-check"></i>Exercise 1: Data Import</a></li>
<li class="chapter" data-level="" data-path="exercise-2-export-data-to-a-file.html"><a href="exercise-2-export-data-to-a-file.html"><i class="fa fa-check"></i>Exercise 2 - Export data to a file</a></li>
<li class="chapter" data-level="" data-path="exercise-3-exploring-the-data.html"><a href="exercise-3-exploring-the-data.html"><i class="fa fa-check"></i>Exercise 3 - Exploring the data</a></li>
<li class="chapter" data-level="" data-path="exercise-4-subsets-of-data-and-logical-operators.html"><a href="exercise-4-subsets-of-data-and-logical-operators.html"><i class="fa fa-check"></i>Exercise 4 - Subsets of data and logical operators</a></li>
<li class="chapter" data-level="" data-path="exercise-5-graphics.html"><a href="exercise-5-graphics.html"><i class="fa fa-check"></i>Exercise 5 - Graphics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regression-analysis.html"><a href="regression-analysis.html"><i class="fa fa-check"></i>Regression Analysis</a><ul>
<li class="chapter" data-level="" data-path="exercise-1.html"><a href="exercise-1.html"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="exercise-2.html"><a href="exercise-2.html"><i class="fa fa-check"></i>Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i>Analysis of variance</a><ul>
<li class="chapter" data-level="" data-path="exercise-four.html"><a href="exercise-four.html"><i class="fa fa-check"></i>Exercise Four</a></li>
<li class="chapter" data-level="" data-path="exercise-five.html"><a href="exercise-five.html"><i class="fa fa-check"></i>Exercise Five</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multivariate-analysis.html"><a href="multivariate-analysis.html"><i class="fa fa-check"></i>Multivariate Analysis</a><ul>
<li class="chapter" data-level="" data-path="exercise-6.html"><a href="exercise-6.html"><i class="fa fa-check"></i>Exercise 6</a></li>
<li class="chapter" data-level="" data-path="exercise-7.html"><a href="exercise-7.html"><i class="fa fa-check"></i>Exercise 7</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="generalized-linear-model.html"><a href="generalized-linear-model.html"><i class="fa fa-check"></i>Generalized Linear Model</a><ul>
<li class="chapter" data-level="" data-path="exercise-7-1.html"><a href="exercise-7-1.html"><i class="fa fa-check"></i>Exercise 7</a></li>
<li class="chapter" data-level="" data-path="exercise-8.html"><a href="exercise-8.html"><i class="fa fa-check"></i>Exercise 8</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mixed-effect-model.html"><a href="mixed-effect-model.html"><i class="fa fa-check"></i>Mixed Effect Model</a><ul>
<li class="chapter" data-level="" data-path="exercise-9.html"><a href="exercise-9.html"><i class="fa fa-check"></i>Exercise 9</a></li>
<li class="chapter" data-level="" data-path="exercise-10.html"><a href="exercise-10.html"><i class="fa fa-check"></i>Exercise 10</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="group-exercises.html"><a href="group-exercises.html"><i class="fa fa-check"></i>Group Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercise-one.html"><a href="exercise-one.html"><i class="fa fa-check"></i>Exercise One</a></li>
</ul></li>
<li class="divider"></li>
<li><a name="title" href="javascript:void(0)"><b>Lecturer:</b></a><ul>
<li><a href="https://www.nmbu.no/ans/solve.saebo" target="blank">Solve Sæbø</a></li></ul></li>
<li><a name="title" href="javascript:void(0)"><b>Assistent Teachers:</b></a><ul>
<li><a href="https://www.nmbu.no/ans/raju.rimal" target="blank">Raju Rimal</a></li></ul></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Methods in Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="exercise-7" class="section level2">
<h2>Exercise 7</h2>
<div id="ex-1-prediction-of-cow-milk-percentage" class="section level3">
<h3>Ex-1: Prediction of cow milk percentage</h3>
<p>In this exercise we will study a data set used in the <a href="http://www.sciencedirect.com/science/article/pii/S0169743909001476">paper</a> by Liland et al (2009) (<code>http://www.sciencedirect.com/science/article/pii/S0169743909001476</code>) were PLS-regression was used to predict the percentage of cow-milk in mixtures of cow, goat and ewe milk.</p>
<p>An excerpt from the paper explains why this is interesting:</p>
<blockquote>
<p>“Quality assurance is an important issue in modern food production. The products are expected to have the right taste, smell, texture and appearance. In addition they should be safe, wholesome, authentic and have a composition that complies with regulations. As a practical example this paper will analyse data simulating milk adulteration. In real life such adulteration could occur where one type of milk is replaced by, or mixed with, another deliberately, by accident or because of failing routines.</p>
</blockquote>
<blockquote>
<p>There are several reasons why detection of the concentrations of cow, goat and ewe milk is of importance. Pure products of goat milk may be used as a supplement of milk for humans who are born with allergic reactions towards cow milk. Some mixed milk products are produced with regard to specifications which specify the mixture of milk from cow, goat and/or ewe. Professionals and consumers want to control the origin of milk in order to be sure that they get products following specifications and labelling. Farmers producing more than one type of milk might be tempted to add cow milk to goat or ewe milk as this would result in higher quantities of the better paid milk variants&quot;</p>
</blockquote>
<p>In the data file “<code>maldidata.rdata</code>” you find four objects:</p>
<ul>
<li><code>Y</code> : The percentage of cow-milk for 4 replicates of 45 different milk mixtures</li>
<li><code>X</code> : Mass Spectrometry data (<code>MALDI-TOF</code>) for the milk samples. The 6179 variables is a quantification of molecule “<code>size</code>” and “<code>charge</code>” in a sample. For simplicity we may say that the size of molecules increases from variable 1 to variable 6179. The measurements are then amounts of molecules of different sizes. The method is used to separate proteins, peptides and other ionizable compunds.</li>
<li><code>Ytest</code> : Cow-milk percentages for 45 extra test samples</li>
<li><code>Xtest</code> : The <code>MALDI-TOF</code> values for the test samples.</li>
</ul>
<p>You may plot the spectra in <code>X</code> by:</p>
<div id="one-by-one" class="section level5">
<h5>One-by-One:</h5>
<p><img src="STAT340-2017_files/figure-html/unnamed-chunk-133-1.png" width="672" /></p>
</div>
<div id="or-all-together-takes-a-moment-to-plot" class="section level5">
<h5>or all together (takes a moment to plot):</h5>
<p><img src="STAT340-2017_files/figure-html/unnamed-chunk-134-1.png" width="672" /></p>
<p>The peaks show molecule sizes that are abundant in the sample.</p>
<ol style="list-style-type: lower-alpha">
<li>First run a PCA on <code>X</code>. How many components are needed to explain 80% and 90% of the variability in <code>X</code>?</li>
</ol>
<div class="ans">
<pre><code>Importance of components:
                          PC1     PC2     PC3     PC4     PC5    PC6
Standard deviation     12.502 4.48331 3.02181 2.53728 2.01120 1.7430
Proportion of Variance  0.648 0.08333 0.03786 0.02669 0.01677 0.0126
Cumulative Proportion   0.648 0.73129 0.76914 0.79584 0.81260 0.8252
                           PC7    PC8     PC9    PC10    PC11    PC12
Standard deviation     1.30637 1.0875 1.05133 0.96666 0.91363 0.84363
Proportion of Variance 0.00708 0.0049 0.00458 0.00387 0.00346 0.00295
Cumulative Proportion  0.83228 0.8372 0.84176 0.84564 0.84910 0.85205
                          PC13    PC14    PC15    PC16    PC17    PC18
Standard deviation     0.83145 0.81638 0.80146 0.78302 0.75653 0.74396
Proportion of Variance 0.00287 0.00276 0.00266 0.00254 0.00237 0.00229
Cumulative Proportion  0.85491 0.85768 0.86034 0.86288 0.86525 0.86755
                          PC19    PC20    PC21    PC22    PC23    PC24
Standard deviation     0.72986 0.72352 0.71682 0.70165 0.68635 0.68112
Proportion of Variance 0.00221 0.00217 0.00213 0.00204 0.00195 0.00192
Cumulative Proportion  0.86976 0.87193 0.87406 0.87610 0.87805 0.87997
                          PC25    PC26    PC27    PC28   PC29    PC30
Standard deviation     0.67526 0.66960 0.66803 0.64452 0.6412 0.63550
Proportion of Variance 0.00189 0.00186 0.00185 0.00172 0.0017 0.00167
Cumulative Proportion  0.88187 0.88372 0.88557 0.88730 0.8890 0.89067
                          PC31   PC32    PC33    PC34    PC35   PC36
Standard deviation     0.63118 0.6211 0.61880 0.61289 0.60748 0.6018
Proportion of Variance 0.00165 0.0016 0.00159 0.00156 0.00153 0.0015
Cumulative Proportion  0.89233 0.8939 0.89551 0.89707 0.89860 0.9001
                          PC37    PC38    PC39    PC40    PC41    PC42
Standard deviation     0.59957 0.59745 0.58987 0.58556 0.58223 0.57872
Proportion of Variance 0.00149 0.00148 0.00144 0.00142 0.00141 0.00139
Cumulative Proportion  0.90159 0.90307 0.90452 0.90594 0.90734 0.90873
                          PC43    PC44    PC45    PC46    PC47    PC48
Standard deviation     0.57270 0.57009 0.56829 0.56415 0.56299 0.55657
Proportion of Variance 0.00136 0.00135 0.00134 0.00132 0.00131 0.00128
Cumulative Proportion  0.91009 0.91144 0.91278 0.91410 0.91541 0.91669
                          PC49    PC50    PC51    PC52    PC53    PC54
Standard deviation     0.55441 0.55249 0.55013 0.54621 0.54236 0.53944
Proportion of Variance 0.00127 0.00127 0.00125 0.00124 0.00122 0.00121
Cumulative Proportion  0.91797 0.91923 0.92049 0.92173 0.92295 0.92415
                          PC55    PC56    PC57    PC58    PC59    PC60
Standard deviation     0.53519 0.53392 0.53038 0.52837 0.52701 0.52362
Proportion of Variance 0.00119 0.00118 0.00117 0.00116 0.00115 0.00114
Cumulative Proportion  0.92534 0.92652 0.92769 0.92885 0.93000 0.93113
                          PC61   PC62   PC63    PC64    PC65    PC66
Standard deviation     0.52164 0.5154 0.5146 0.51075 0.50803 0.50581
Proportion of Variance 0.00113 0.0011 0.0011 0.00108 0.00107 0.00106
Cumulative Proportion  0.93226 0.9334 0.9345 0.93554 0.93661 0.93767
                          PC67    PC68    PC69   PC70    PC71    PC72
Standard deviation     0.50305 0.50060 0.49479 0.4904 0.48717 0.48528
Proportion of Variance 0.00105 0.00104 0.00101 0.0010 0.00098 0.00098
Cumulative Proportion  0.93872 0.93976 0.94078 0.9418 0.94276 0.94373
                          PC73    PC74    PC75    PC76    PC77   PC78
Standard deviation     0.48157 0.47981 0.47502 0.47299 0.46736 0.4655
Proportion of Variance 0.00096 0.00095 0.00094 0.00093 0.00091 0.0009
Cumulative Proportion  0.94470 0.94565 0.94658 0.94751 0.94842 0.9493
                          PC79    PC80    PC81    PC82    PC83    PC84
Standard deviation     0.46270 0.46029 0.45732 0.45434 0.45185 0.44991
Proportion of Variance 0.00089 0.00088 0.00087 0.00086 0.00085 0.00084
Cumulative Proportion  0.95020 0.95108 0.95195 0.95281 0.95365 0.95449
                          PC85    PC86   PC87    PC88    PC89    PC90
Standard deviation     0.44596 0.44150 0.4392 0.43659 0.43298 0.43008
Proportion of Variance 0.00082 0.00081 0.0008 0.00079 0.00078 0.00077
Cumulative Proportion  0.95532 0.95612 0.9569 0.95771 0.95849 0.95926
                          PC91    PC92    PC93    PC94    PC95    PC96
Standard deviation     0.42631 0.42429 0.42329 0.42076 0.41632 0.41304
Proportion of Variance 0.00075 0.00075 0.00074 0.00073 0.00072 0.00071
Cumulative Proportion  0.96001 0.96076 0.96150 0.96223 0.96295 0.96366
                          PC97   PC98    PC99   PC100   PC101   PC102
Standard deviation     0.41239 0.4099 0.40814 0.40579 0.40288 0.40032
Proportion of Variance 0.00071 0.0007 0.00069 0.00068 0.00067 0.00066
Cumulative Proportion  0.96437 0.9651 0.96575 0.96644 0.96711 0.96777
                         PC103   PC104   PC105   PC106   PC107   PC108
Standard deviation     0.39830 0.39665 0.39492 0.39189 0.39021 0.38745
Proportion of Variance 0.00066 0.00065 0.00065 0.00064 0.00063 0.00062
Cumulative Proportion  0.96843 0.96908 0.96973 0.97037 0.97100 0.97162
                         PC109   PC110   PC111  PC112   PC113   PC114
Standard deviation     0.38632 0.38379 0.38268 0.3806 0.37875 0.37773
Proportion of Variance 0.00062 0.00061 0.00061 0.0006 0.00059 0.00059
Cumulative Proportion  0.97224 0.97285 0.97346 0.9741 0.97465 0.97524
                         PC115   PC116   PC117   PC118   PC119   PC120
Standard deviation     0.37614 0.37361 0.36939 0.36727 0.36596 0.36319
Proportion of Variance 0.00059 0.00058 0.00057 0.00056 0.00056 0.00055
Cumulative Proportion  0.97583 0.97641 0.97697 0.97753 0.97809 0.97864
                         PC121   PC122   PC123   PC124   PC125   PC126
Standard deviation     0.36144 0.35951 0.35838 0.35604 0.35339 0.34950
Proportion of Variance 0.00054 0.00054 0.00053 0.00053 0.00052 0.00051
Cumulative Proportion  0.97918 0.97971 0.98025 0.98077 0.98129 0.98179
                        PC127  PC128   PC129   PC130   PC131   PC132
Standard deviation     0.3485 0.3466 0.34274 0.34143 0.33990 0.33821
Proportion of Variance 0.0005 0.0005 0.00049 0.00048 0.00048 0.00047
Cumulative Proportion  0.9823 0.9828 0.98328 0.98377 0.98425 0.98472
                         PC133   PC134   PC135   PC136   PC137   PC138
Standard deviation     0.33513 0.33150 0.32945 0.32701 0.32469 0.32042
Proportion of Variance 0.00047 0.00046 0.00045 0.00044 0.00044 0.00043
Cumulative Proportion  0.98519 0.98564 0.98609 0.98653 0.98697 0.98740
                         PC139   PC140   PC141  PC142   PC143   PC144
Standard deviation     0.31923 0.31695 0.31344 0.3102 0.30820 0.30597
Proportion of Variance 0.00042 0.00042 0.00041 0.0004 0.00039 0.00039
Cumulative Proportion  0.98782 0.98824 0.98864 0.9890 0.98944 0.98982
                         PC145   PC146   PC147   PC148   PC149   PC150
Standard deviation     0.30490 0.30139 0.29943 0.29712 0.29419 0.29282
Proportion of Variance 0.00039 0.00038 0.00037 0.00037 0.00036 0.00036
Cumulative Proportion  0.99021 0.99059 0.99096 0.99132 0.99168 0.99204
                         PC151   PC152   PC153   PC154   PC155   PC156
Standard deviation     0.29097 0.28744 0.28654 0.28428 0.28340 0.27948
Proportion of Variance 0.00035 0.00034 0.00034 0.00034 0.00033 0.00032
Cumulative Proportion  0.99239 0.99273 0.99307 0.99341 0.99374 0.99406
                         PC157   PC158   PC159  PC160  PC161   PC162
Standard deviation     0.27619 0.27450 0.27344 0.2704 0.2689 0.26642
Proportion of Variance 0.00032 0.00031 0.00031 0.0003 0.0003 0.00029
Cumulative Proportion  0.99438 0.99469 0.99500 0.9953 0.9956 0.99590
                         PC163   PC164   PC165   PC166   PC167   PC168
Standard deviation     0.26437 0.26064 0.25931 0.25591 0.25432 0.25173
Proportion of Variance 0.00029 0.00028 0.00028 0.00027 0.00027 0.00026
Cumulative Proportion  0.99619 0.99647 0.99675 0.99702 0.99729 0.99755
                         PC169   PC170   PC171   PC172   PC173   PC174
Standard deviation     0.24955 0.24744 0.24511 0.24292 0.23712 0.23583
Proportion of Variance 0.00026 0.00025 0.00025 0.00024 0.00023 0.00023
Cumulative Proportion  0.99781 0.99806 0.99831 0.99856 0.99879 0.99902
                         PC175   PC176   PC177   PC178   PC179
Standard deviation     0.23060 0.22685 0.22259 0.21212 0.19166
Proportion of Variance 0.00022 0.00021 0.00021 0.00019 0.00015
Cumulative Proportion  0.99924 0.99946 0.99966 0.99985 1.00000
                                     PC180
Standard deviation     0.00000000000000235
Proportion of Variance 0.00000000000000000
Cumulative Proportion  1.00000000000000000</code></pre>
<p>We need 5 components to explain 80% and 36 to explain 90%. Hopefully we do not need to use all X-information to predict <code>Y</code>.</p>
</div>
<ol start="2" style="list-style-type: lower-alpha">
<li>Compute the correlations between <span class="math inline">\(Y\)</span> and the principal components. How can you use this get an idea of how many components PLS-regression will require to make a good model for cow-milk prediction?</li>
</ol>
<div class="ans">
<pre><code>           PC1        PC2        PC3        PC4        PC5       PC6
[1,] 0.5282887 -0.7537702 0.03383417 -0.2250967 0.07440021 -0.050276
           PC7        PC8          PC9       PC10       PC11        PC12
[1,] 0.0150393 -0.1656208 -0.007569666 -0.0754223 -0.1365365 -0.04318635
           PC13       PC14        PC15        PC16       PC17        PC18
[1,] 0.01833581 0.01665996 -0.01545572 -0.02629766 0.02219768 -0.01873644
             PC19        PC20        PC21        PC22        PC23
[1,] -0.004954056 -0.04133758 0.006487258 -0.01874077 -0.04420428
             PC24        PC25        PC26        PC27       PC28
[1,] -0.006899854 -0.04110336 0.004122647 -0.01226996 0.01698066
            PC29       PC30         PC31       PC32       PC33       PC34
[1,] -0.01472012 0.03866793 0.0006183768 0.01852025 -0.0213186 0.04777714
             PC35        PC36         PC37       PC38        PC39
[1,] -0.009640993 -0.01055468 -0.005027939 0.02095968 -0.01101386
             PC40        PC41        PC42      PC43        PC44      PC45
[1,] -0.008739908 -0.01910303 -0.01403824 0.0081989 -0.01239098 0.0175803
            PC46       PC47        PC48       PC49        PC50
[1,] 0.005968477 0.01408276 -0.01929257 0.02167754 -0.00107033
             PC51        PC52        PC53         PC54        PC55
[1,] -0.004538278 0.007807648 0.006737832 -0.004095661 0.002704891
             PC56        PC57        PC58         PC59          PC60
[1,] -0.004040486 -0.02687466 0.005213355 -0.004714854 -0.0005855282
           PC61        PC62      PC63        PC64         PC65        PC66
[1,] 0.01240884 -0.01415087 0.0124614 -0.01805413 -0.007655923 0.002764712
           PC67        PC68         PC69         PC70         PC71
[1,] 0.01456288 -0.02207951 -0.002453118 -0.003848137 -0.003343055
             PC72        PC73        PC74        PC75       PC76
[1,] -0.004034935 -0.03796665 -0.03938474 0.007020675 0.01030672
            PC77        PC78        PC79         PC80        PC81
[1,] -0.01699946 -0.01035674 -0.01159898 -0.008952226 0.006640257
             PC82        PC83        PC84       PC85       PC86       PC87
[1,] -0.007161444 0.008595261 -0.02049167 -0.0217142 0.01803896 0.02837337
            PC88       PC89        PC90         PC91        PC92
[1,] 0.006726205 0.01657023 0.004445855 -0.006956301 -0.01087622
            PC93       PC94       PC95        PC96         PC97       PC98
[1,] -0.00113085 0.02443427 0.03041671 -0.01861646 -0.003882029 0.02139643
             PC99        PC100        PC101      PC102        PC103
[1,] -0.007068423 -0.002379111 0.0002831574 0.02506361 -0.007573581
           PC104       PC105      PC106       PC107      PC108       PC109
[1,] -0.01748087 -0.02588017 0.02444957 -0.02602153 0.00271879 -0.01570387
          PC110       PC111        PC112      PC113      PC114       PC115
[1,] 0.01418877 -0.01153658 -0.009002262 -0.0115408 0.02989033 0.009244466
           PC116       PC117       PC118       PC119       PC120
[1,] 0.002463251 -0.01004417 -0.00906929 -0.02419253 0.005161842
          PC121      PC122       PC123       PC124       PC125       PC126
[1,] -0.0065995 0.01469365 0.004668753 -0.02539656 -0.02236522 -0.01235845
           PC127      PC128       PC129       PC130         PC131
[1,] 0.008160952 0.01009865 -0.01423243 -0.01190733 -0.0001797449
           PC132       PC133       PC134        PC135      PC136
[1,] -0.01077575 -0.00171661 0.002751793 -0.006353652 0.01025625
            PC137       PC138       PC139        PC140        PC141
[1,] -0.008700718 0.005517349 0.007418038 -0.008046314 -0.003271586
          PC142        PC143       PC144        PC145        PC146
[1,] 0.01592453 -0.001092543 0.002911843 -0.003775066 -0.001475013
            PC147        PC148       PC149        PC150       PC151
[1,] -0.007356669 -0.006561469 0.003542154 -0.005788942 0.003402721
           PC152       PC153       PC154       PC155      PC156      PC157
[1,] 0.002515702 0.001500009 -0.00918038 -0.01106579 0.02320501 0.01080684
            PC158        PC159       PC160        PC161        PC162
[1,] -0.007336685 -0.001484081 0.005028354 -0.002423504 -0.005832478
            PC163       PC164        PC165       PC166       PC167
[1,] -0.007009756 0.005582993 -0.003444765 0.004363855 0.003676115
           PC168      PC169        PC170       PC171        PC172
[1,] 0.005293779 -0.0150235 -0.009197677 -0.01335412 -0.009331149
            PC173      PC174        PC175       PC176        PC177
[1,] -0.002554266 0.01128622 0.0002403935 -0.00041784 -0.005701346
           PC178     PC179      PC180
[1,] -0.01030182 0.0217922 0.02875971</code></pre>
<p>Components 1, 2, 4 and perhaps 8 are moderately to highly correlated with <code>Y</code>. Supposedly 3 to 4 components will be necessary for PLSR.</p>
</div>
<ol start="3" style="list-style-type: lower-alpha">
<li>Fit a PLS-regression model using <code>Y</code> as response and <code>X</code> as predictor (You may simply write <code>Y ~ X</code> as your model call in <code>plsr</code>. Also use <code>ncomp=10</code> as extra argument to only fit 1 to 10 components). Use the <code>scoreplot()</code> function to make a scoreplot. Check the help-file for this function to see how you can chose the component numbers to plot, and how you can put labels to your observations. Plot component 1 against 2 and put observation numbers 1:180 as labels. If the noise level of the measurements is low, the replicates should group in clusters of size four (obs 1,2,3 and 4 are from the same mixture, and so on). Do you see any tendency to this?</li>
</ol>
<div class="ans">
<p><img src="STAT340-2017_files/figure-html/unnamed-chunk-137-1.png" width="672" /></p>
<p>We observe that there are clusters of observations and that there is a tendency of successive numbers to be close in the scoreplot. Cluster sizes are difficult to determine.</p>
</div>
<ol start="4" style="list-style-type: lower-alpha">
<li>Perform hierarchical clustering of the samples using the 3 first PLS-component scores as input variables. Try both “complete” and “average” agglomeration method and make a dendrogram. Are the replicate clusters more apparent in the dendrogram than in the scoreplot? Is there any samples that are very different from all others?</li>
</ol>
<div class="ans">
<p><img src="STAT340-2017_files/figure-html/unnamed-chunk-138-1.png" width="672" /></p>
<p>From the dendrogram we observe several clusters of size four, and some of three with successive observation numbers. This implies that the replicates are more similar to each other than samples from different milk samples. Sample 159 seems to be an outlier, very different from all others.</p>
</div>
<ol start="5" style="list-style-type: lower-alpha">
<li>Use K-means clustering with K=45 on the three first PLS-component scores. How do the replicates cluster?</li>
</ol>
<div class="ans">
<pre><code>  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 
 39  39  39  39  13  13  13  44  20  20  20  20  31  31  31  31  38  15 
 19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36 
 38  15  13   1   1  40  25  25  14  17   2   2   2   2  22  32  22  32 
 37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54 
 25  17  25  25  44  23   9  34   3   3   3   3  11  11  19  23  17  24 
 55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72 
 24  24  30  30  14  14  40  23  40  39   9   9   9   9   1  30   1  13 
 73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90 
 12  45  45  45  44  44  33  22   5  13  12  35  34  34  34  23  14  17 
 91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 
 17  17   6   6  28  28  42  42  42  42  22  32  43  43   7   7   7   7 
109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 
 38  38  38  38   4   4   4   4  37  37  37  37  41   8   8  41  29  29 
127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 
 29  26  25  16  16  16  25  41  25  25  35   5   5  35  42  42  42  42 
145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 
 21  21  21  21  36  36  36  36  18  18  18  18  33   6  27  28   8   8 
163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 
  8  30  14  41  41  16  12  45  45  35  10  30  10  10  15  15   3  15 </code></pre>
<p>Many replicates fall into the same clusters, but some clusters observations from more than one “true” cluster. E.g. cluster number 43 contains observations (97, 98, 99, 100) and (141, 142, 143, 144), which is a mixture of four true replica clusters. We conclude that the replicates are somewhat similar, but there is some noise in the <code>MALDI-TOF</code> data which makes similar milk mixtures hard to distinguish.</p>
</div>
<ol start="6" style="list-style-type: lower-alpha">
<li>We will use cross-validation to estimate the expected prediction error of PLSR and to choose the appropriate number of components. Instead of using Leave-One-Out Cross-validation we will exclude all four replicates in a “Leave-Four-Out” type of Cross-validation. Why is this smart?</li>
</ol>
<div class="ans">
<p>Since the replicates are from the same mixture, they are not independent. If we use Leave-One-Out CV the three replicates still contained in the training set will make the model “too good” to predict the given mixture, and we will under-estimate the prediction error of new samples.</p>
</div>
<ol start="7" style="list-style-type: lower-alpha">
<li>Refit the plsmodel from exercise c. but add the arguments <code>validation=&quot;CV&quot;</code>, <code>segments=45</code> and <code>segment.type=&quot;consecutive&quot;</code> in the plsr-model call. This sets up the “Leave-Four-Out” CV to be performed.</li>
</ol>
<div class="ans">

</div>
<ol start="8" style="list-style-type: lower-alpha">
<li>The sum of squared prediction errors (PRESS) for different number of components is given in the <code>validation$PRESS</code> element of the fitted pls-model. The null-model PRESS (prediction using the mean-response) is given in the <code>validation$PRESS0</code> element. You find the MSEP-values (Mean Squared Error of Prediction) by dividing the PRESS by the number of observations (N=180). Make a prediction error plot of MSEP with 0 to 10 components. How many components do you think gives satisfying prediction of cow-milk content in new mixtures? Remember that simple models are often better than complex.</li>
</ol>
<div class="ans">
<p><img src="STAT340-2017_files/figure-html/unnamed-chunk-142-1.png" width="672" /></p>
<p>The prediction error is heavily reduced as we introduce components 1 and 2, but there is a small gain by adding components 3, 4 and 5. Simple models are usually more robust, so I would not go any further than 5 components here.</p>
</div>
<ol style="list-style-type: lower-roman">
<li>Predict the cow-milk content of the test samples using <code>Xtest</code> as newdata in the <code>predict</code>-function and use the number of components you found as best from the previous exercise. Save the predictions into an object called <code>pred</code>. (See <code>?predict.mvr</code> for the help-file to predict for pls-objects.). The predict-function returns an array of dimension [45,1,1] of predictions. You can extract all predictions by <code>pred[,1,1]</code>.</li>
</ol>
<div class="ans">

</div>
<p>Copy the folowing code into R and execute:</p>
<ol start="10" style="list-style-type: lower-alpha">
<li>Use <code>MSEPfunc()</code> to compute the MSEP-value for the test-predictions using <code>Ytest</code> and the predicted values as inputs. Did the cross-validation under-estimate the prediction error?</li>
</ol>
<div class="ans">
<pre><code>[1] 0.006919722</code></pre>
<p>The MSEP-value is slightly larger than the value found for five components using Cross-validation, but the order of magnitude is the same.</p>
</div>
<ol start="11" style="list-style-type: lower-alpha">
<li>EXTRA for those interested. Redo the exercises g. and h. with Leave-One-Out CV. (See also lecture notes Lesson 7.) Compare the MSEP-values you find with the previous CV-routine.</li>
</ol>
<div class="ans">
<p><img src="STAT340-2017_files/figure-html/unnamed-chunk-146-1.png" width="672" /></p>
<p>We see that LOO-CV underestimates the prediction error, as commented in exercise f.</p>
</div>

</div>
</div>
</div>
<!-- </div> -->
<script>
ans_btn_html = '<a class = "btn-ans"><span class="label label-success no-print">&lt;ANS/&gt;</span></a>';
  showAns_btn = '<a class="showAns-btn btn pull-right js-toolbar-action">Show Answers</a>';
  hideAns_btn = '<a class="hideAns-btn btn pull-right js-toolbar-action" style="display:none;">Hide Answers</a>';
  
  // Adding Extra Menus in Navigation
  
  // Adding button on before each answer:::
  $(".ans").before(ans_btn_html);
  $('.btn-ans').each(function(){
    $(this).next('.ans').andSelf().wrapAll('<div class="wrap-ans"/>');
  });
  $('.btn-ans').click(function(){
    $(this).siblings('.ans').toggle(400);
  });
  
if ($(".ans").length > 0){
    $('.book-header').prepend(showAns_btn, hideAns_btn);
    $('.showAns-btn').show();
    $('.showAns-btn').click(function(){
      $('.btn-ans').hide(400);
      $('.ans').show(400);
      $('.hideAns-btn').show();
      $(this).hide();
      $('.hideAns-btn').click(function(){
        $('.btn-ans').show(400);
        $('.ans').hide(400);
        $('.showAns-btn').show();
        $(this).hide();
      });
    });
  }

var chap = "week-two"
if(window.location.href.indexOf(chap) != -1){
  var now = new Date();
  var release = new Date("August 21, 2016");
  if(now < release){
    $(".ans, .btn-ans, .showAns-btn, .hideAns-btn").hide()
  }
}

$('.ans p:has(img)').css('text-align', 'center');
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82812028-1', 'auto');
  ga('send', 'pageview');

</script>
            </section>

          </div>
        </div>
      </div>
<a href="exercise-6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generalized-linear-model.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["STAT340-2017.pdf", "STAT340-2017.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
